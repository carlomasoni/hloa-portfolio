name: CI

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:

jobs:
  build-test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.12", "3.13"]

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'

      - name: Create and activate virtual environment
        run: |
          python -m venv venv
          echo "VIRTUAL_ENV=$PWD/venv" >> $GITHUB_ENV
          echo "$PWD/venv/bin" >> $GITHUB_PATH

      - name: Install project dependencies
        run: |
          python -m pip install --upgrade pip
          python -m pip install -e .[dev]

      - name: Dependency check
        run: python dep_check.py

      - name: Run HLOA benchmark tests
        run: python test_hloa_benchmarks.py

      - name: Run unit tests
        run: python -m pytest -q

      - name: Run linting
        run: |
          python -m ruff check src/ tests/
          python -m black --check src/ tests/

  benchmark-test:
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python 3.13
        uses: actions/setup-python@v5
        with:
          python-version: "3.13"
          cache: 'pip'

      - name: Create and activate virtual environment
        run: |
          python -m venv venv
          echo "VIRTUAL_ENV=$PWD/venv" >> $GITHUB_ENV
          echo "$PWD/venv/bin" >> $GITHUB_PATH

      - name: Install project dependencies
        run: |
          python -m pip install --upgrade pip
          python -m pip install -e .[dev]

      - name: Run comprehensive HLOA benchmarks
        run: |
          python -c "
          import sys
          sys.path.append('src')
          sys.path.append('benchmarks')
          from benchmark_runner import BenchmarkRunner
          from hloa.core import HLOA_Config
          
          config = HLOA_Config(pop_size=30, iters=100, seed=42)
          runner = BenchmarkRunner(config)
          results = runner.run_benchmark_suite(
              functions=['sphere', 'rastrigin', 'ackley', 'griewank'],
              dimensions=[10, 30],
              runs_per_function=3
          )
          runner.print_summary(results)
          "

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: benchmark-results
          path: hloa_benchmark_results.json